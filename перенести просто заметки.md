> Основной реп, до этого делал по форку какому-то непонятному
>
> `https://github.com/wg-easy/wg-easy/tree/master`
>
> Всякие варики по панелям/скриптам для WG
> 
> `https://habr.com/ru/articles/738890/`

---

Пароль работает только в .env файле, иначе не але =)

В `compose.yml` проброс портов с указанием ip работает только с ip-адресами, dns имена указывать нельзя.

так можно:
```yaml
    ports:
      - "30301:30301/udp"
      - "127.0.0.1:30302:30302/tcp"
```

так нельзя:
```bash
    ports:
      - "30301:30301/udp"
      - "localhost:30302:30302/tcp"
```

---

Посмотреть свой внешний IP на лине:
```bash
wget -qO- ident.me
```

---

Прикол: при объявлении сети в compose файле, все контейнеры которые в ней находится, смогут общаться друг с другом посредством hostname'ов.
То есть, если указан хостнейм при создании контейнера - то можно просто сделать `ping <hostname>`, все будет ок =)

В этом примере nginx и httpd смогут общаться. По `httpd1` и `nginx` соотвественно. 

```yaml
version: '3.9'

name: proxy-and-app

services:
  httpd1:
    image: httpd
    hostname: httpd1

  nginx:
    image: nginx
    hostname: nginx
```

Также, полное доменное имя будет типа: `<имя_контейнера.имя_сети>`. То есть, они могут общаться по:
- имени контейнера (в самом докере, `docker ps -a`)
- полному доменному имени
- hostname'у контейнера

---

Самоподписанный серт в nginx:
> https://www.8host.com/blog/samopodpisannyj-ssl-sertifikat-dlya-nginx/?ysclid=m5smnr3v3v465278232

---

- Адрес сети в докере - не изменить (пока не нашел варианта). Проще использовать адресацию по именам, она куда удобнее.
- `docker-compose config` - покажет итоговую конфигурацию, включая все переопределения и параметры, которые задаются по умолчанию.
Изначально:
```yaml
version: '3.9'

name: proxy-and-app

services:
  httpd1:
    image: httpd
    hostname: httpd1

  nginx:
    image: nginx
    hostname: nginx
    
networks:
  default:
    driver: bridge
```
В выводе `docker-compose config`:
```yaml
name: proxy-and-app
services:
  httpd1:
    hostname: httpd1
    image: httpd
    networks:
      default: null
  nginx:
    hostname: nginx
    image: nginx
    networks:
      default: null
networks:
  default:
    name: proxy-and-app_default
    driver: bridge
```

---

Дока с конфигами для nginx:
> https://docs.nginx.com/nginx/admin-guide/security-controls/securing-http-traffic-upstream/#configuring-nginx

---

Базовый набор для тестов в контейнере (debian):

```bash
apt update && apt install -y \
  bash bash-completion nano \
  htop openrc \
  iproute2 iputils-ping ssh curl wget
```

---

В конфигурации nginx, при указании цели для проксирования есть интересный момент:
- Если в следующей секции у `proxy_pass` в конце не ставить `/` - то прокси будет искать на хосте httpd1 путь httpd1.
```conf
    location /httpd1 {
        proxy_pass http://httpd1;
    }
```
- Если этот слеш поставить - то уже при переходе на самом nginx на `https://nginx-name/httpd1` он будет просто перенаправлять на httpd1, в его корень. Такая история.

---

Сеть в докере со своим адресом все таки можно создать, как и в compose, так и в самом докере из CLI.
Compose:
```yaml
networks:
  default:
    name: net
    ipam:
      config:
        - subnet: 10.110.0.0/16
```
CLI:
```bash
docker network create \
  --driver bridge \
  --subnet 192.168.0.0/24 \
  --ip-range 192.168.0.0/24 \
  --gateway 192.168.0.1 \
  --internal \
  my_network
```


---

Основные настройки сети в compose файле:
```yaml
networks:
  my_network:
    driver: bridge                    # Указывает драйвер сети (bridge, overlay, macvlan и т.д.)
    driver_opts:                      # Опции для драйвера
      com.docker.network.bridge.name: my_bridge   # Указывает имя для сетевого моста
    ipam:                             # Управление IP-адресами
      config:
        - subnet: 192.168.0.0/24       # Задает подсеть
          ip_range: 192.168.0.0/24     # Задает диапазон IP для присвоения
          gateway: 192.168.0.1         # Указывает шлюз
    internal: true                     # Изолирует сеть от внешнего доступа
    attachable: true                   # Позволяет подключать внешние контейнеры к сети
    enable_ipv6: true                  # Включает поддержку IPv6
    labels:                            # Установить метки для сети
      my_label: "my_value"             # Применение меток
    options:                           # Дополнительные опции для драйвера (например, для macvlan)
      parent: eth0                     # Указывает родительский интерфейс для macvlan
```

---

На некоторых системах напрочь отказывается работать `bash-completion`, типа Ubuntu и Debian частенько бывает. Чтобы поправить, надо в `.bashrc` добавить
```bash
if ! shopt -oq posix; then
  if [ -f /usr/share/bash-completion/bash_completion ]; then
    . /usr/share/bash-completion/bash_completion
  elif [ -f /etc/bash_completion ]; then
    . /etc/bash_completion
  fi
fi
```

---

> Решение проблемы с прокси в `kanboard` - https://github.com/kanboard/kanboard/issues/4119#issuecomment-709972726
>
> Решенние проблемы с прокси в `bookstack` - https://github.com/linuxserver/docker-bookstack?tab=readme-ov-file#parameters (смотрим табличку и `-e APP_URL`)

Чтобы использовать обратный прокси для разделения нескольких сайтов, необходимо чуть-чуть подзапариться.

Теория: при записи в конфиге nginx'a `location /тыры-пыры/`, nginx будет действительно выдавать ответы от сервера, который находится по адресу, который указан в `proxy_pass http://site:80/`. 

Данный вариант без дополнительных манипуляций у меня завелся только лишь с `wg-easy`. Вероятно потому, что панелька одностраничная, и никаких дополнительных путей в url нет. 

Переходя к более сложным панелям и сайтам, возникают сложности. В варианте описанном выше, nginx при получении каких-либо путей к другим файлам и страницам на сайте, будет получать url, который не будет учитывать того, что мы указали в `location` на прокси. Чтобы это исправить, нужно настраивать конкретный сервис на дополнительный путь в url. 

Пример: 
В `default.conf` у nginx'a такая запись:
```json
location /kanboard/ {
    proxy_pass http://kanboard:80;
}
```

При переходе на `https://proxy-ip/kanboard/`:

![image](https://github.com/user-attachments/assets/9c2796bc-2576-4535-a031-55237efbd240)


Получим вот такой ответ:

![image](https://github.com/user-attachments/assets/ba3c6bcb-a3f1-4416-a3e5-2c7a10fdaf4b)

В логах у nginx'a будут такие записи:
```
10.8.6.58 - - [18/Jan/2025:22:18:15 +0000] "GET /kanboard/ HTTP/1.1" 302 5 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" "-"
10.8.6.58 - - [18/Jan/2025:22:18:15 +0000] "GET /login HTTP/1.1" 404 555 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" "-"
```
Кратко, процесс будет следуюший:

1. Запрос на `https://proxy-ip/kanboard/` приходит на nginx, после чего он запрашивает данные от `http://kanboard`. 

2. Веб-сервер генерирует ответ `https://proxy-ip/login`, предполагая, что доступен по адресу `https://proxy-ip/`. 

3. Ответ доходит до клиента и браузер понимает, что ему надо открыть `https://proxy-ip/login`. 

4. Nginx снова получает запрос, но уже на открытие `https://proxy-ip/login`. Обрабатывает его, и, увы - он не понимает, что теперь ему этот ответ надо отправить kanboard'у (так как переадресация на kanboard происходит только если путь начинается с `/kanboard`), и соотвественно пытается искать `/login` локально. 

Исправлется это настройкой URL на самом веб-сервере. Он должен понимать, что доступен по адресу `https://proxy-ip/kanboard/` и все ответы ему надо генерировать в соотвествие с этим URL. Например, в kanboard'e это настраивается через `config.php`:
```php
<?php
defined('ENABLE_URL_REWRITE') or define('ENABLE_URL_REWRITE', true); // Стандартные записи
defined('LOG_DRIVER') or define('LOG_DRIVER', 'system'); // Стандартные записи
define('KANBOARD_URL', 'https://10.6.1.62/kanboard/'); // Запись, которая задает этот самый URL
```

В `bookstack`, при реализации через docker, это исправляется переменной `APP_URL`.

---

На VPS при долгом использовании могут скапливаться логи. Обычно в `/var/log/journal`. За несколько месяцев у меня скопился почти гигабайт логов.

Почистить можно через:
1. `journalctl --vacuum-time=10d` - сохранит логи за последние 10 дней.
2. `journalctl --vacuum-size=100M` - сохранит последние логи, которые вместятся в 100 МБ.

Настроить чистку автоматом:
- `mkdir /etc/systemd/journald.conf.d/`, если не создана.
- `nano /etc/systemd/journald.conf.d/clear-log.conf`, в него вставить:
```conf
[Journal]
SystemMaxUse=100M
MaxRetentionSec=1d
```
- `systemctl restart systemd-journald` - перезагрузить сервис для логов
- Проверить можно командой `systemd-analyze cat-config systemd/journald.conf`, она покажет что конфига применилась

> Удобно посмотреть директории, в которых лежит всякое говно, можно через `ncdu` (`apt install ncdu -y`). 
>
> Запускать лучше из корня, просканит - и ползать, смотреть где че лежит.

---

Создать файл диска, создать ФС:
```bash
truncate --size 1G file.img
mkfs.ext4 file.img
```

Смонтировать в `fstab`:
```config
/home/user/file.img /mnt/loop1 ext4 defaults 0 0
```

Смонтировать ручками:
```bash
mount file.img /mnt/loop0
```

---
